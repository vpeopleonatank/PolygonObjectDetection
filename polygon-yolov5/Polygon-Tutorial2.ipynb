{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Tutorial 2\n",
    "Using data **segmentation** and module ***shapely*** to transform dataset to polygon boxes.\n",
    "<br> COCO dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pycocotools\n",
    "import shapely\n",
    "import shapely.geometry\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import repeat\n",
    "from tqdm import tqdm\n",
    "from pycocotools import coco, mask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "oneD2twoD = lambda x: [(x[2*i], x[2*i+1]) for i in range(len(x)//2)]   # one D [x, y, x, y, x, y, ...] to [(x, y), (x, y), ...]\n",
    "catid_to_idx = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, \n",
    "               10: 9, 11: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, \n",
    "               18: 16, 19: 17, 20: 18, 21: 19, 22: 20, 23: 21, 24: 22, \n",
    "               25: 23, 27: 24, 28: 25, 31: 26, 32: 27, 33: 28, 34: 29, \n",
    "               35: 30, 36: 31, 37: 32, 38: 33, 39: 34, 40: 35, 41: 36, \n",
    "               42: 37, 43: 38, 44: 39, 46: 40, 47: 41, 48: 42, 49: 43, \n",
    "               50: 44, 51: 45, 52: 46, 53: 47, 54: 48, 55: 49, 56: 50, \n",
    "               57: 51, 58: 52, 59: 53, 60: 54, 61: 55, 62: 56, 63: 57, \n",
    "               64: 58, 65: 59, 67: 60, 70: 61, 72: 62, 73: 63, 74: 64, \n",
    "               75: 65, 76: 66, 77: 67, 78: 68, 79: 69, 80: 70, 81: 71, \n",
    "               82: 72, 84: 73, 85: 74, 86: 75, 87: 76, 88: 77, 89: 78, 90: 79}\n",
    "\n",
    "\n",
    "def seg2poly(dataset_path='',\n",
    "             plot=True,):\n",
    "    \"\"\"\n",
    "        Transform segmentation to polygon labels (x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Search for COCO json annotation files\n",
    "    f = []  # json files\n",
    "    p = Path(dataset_path)\n",
    "    assert p.is_dir(), \"'dataset_path' should be a valid path.\"\n",
    "    f += glob.glob(str(p / '**' / '*.json'), recursive=True)\n",
    "    assert f, f\"Error: no searched annotations files (.json) with {dataset_path}\"\n",
    "    \n",
    "    # Iterate through each json_file\n",
    "    for json_file in f:\n",
    "        coco_data = pycocotools.coco.COCO(json_file) # load coco data\n",
    "        \n",
    "        parent_path = Path(json_file).parent.parent\n",
    "        img_txt = [] # store searched image files\n",
    "        img_dir = parent_path / 'images'\n",
    "        \n",
    "        # Get prefix name\n",
    "        for file in os.listdir(str(img_dir)):\n",
    "            if file in json_file:\n",
    "                prefix = file\n",
    "                break\n",
    "        \n",
    "        img_dir = img_dir / prefix\n",
    "        anno_dir = parent_path / 'labels' / prefix\n",
    "        if not os.path.exists(str(anno_dir)): os.mkdir(str(anno_dir))\n",
    "        \n",
    "        print(f'Begin transformation for {prefix}')\n",
    "        plot_now = 0\n",
    "        for img_i, img in enumerate(tqdm(coco_data.dataset['images'])):\n",
    "            \n",
    "            plot = plot and plot_now<3 # test the first three images that are not crowded\n",
    "            img_name = img_dir / img['file_name']\n",
    "            if img_name.exists():\n",
    "                anno_name = anno_dir / (os.path.splitext(img['file_name'])[0]+'.txt')\n",
    "                anno_txt = [] # store label information\n",
    "\n",
    "                img_txt.append(str(img_name)) # store current image file\n",
    "                if img['id'] not in coco_data.imgToAnns.keys():\n",
    "                    np.savetxt(str(anno_name), np.array(anno_txt))\n",
    "                    continue\n",
    "\n",
    "                if plot: polygon_coords, segment_coords = [], [] # for plot\n",
    "\n",
    "                # iterate through each object\n",
    "                for object0 in coco_data.imgToAnns[img['id']]:\n",
    "\n",
    "                    # if not crowded, use segmentation\n",
    "                    if not object0['iscrowd']:\n",
    "                        segments = [oneD2twoD(segment+segment[:2]) \n",
    "                                           for segment in object0['segmentation']]\n",
    "                        if plot: \n",
    "                            polygon_coords.append([])\n",
    "                            segment_coords.append([])\n",
    "\n",
    "                    # if crowded, use bbox\n",
    "                    else: \n",
    "                        # x1, y1, x1, y2, x2, y2, x2, y1\n",
    "                        label = [catid_to_idx[object0['category_id']],\n",
    "                                 object0['bbox'][0], object0['bbox'][1], object0['bbox'][0], object0['bbox'][3],\n",
    "                                 object0['bbox'][2], object0['bbox'][3], object0['bbox'][2], object0['bbox'][0]] \n",
    "\n",
    "                        label = normalize_anchors(label, img['height'], img['width'])[0] # normalize xyxyxyxy\n",
    "                        anno_txt.append(label)\n",
    "                        continue\n",
    "\n",
    "                    # iterate through each segmentation\n",
    "                    for segment in segments:\n",
    "\n",
    "                        #using shapely::minimum_rotated_rectangle to convert segmentation\n",
    "                        multipoint = shapely.geometry.MultiPoint(segment)\n",
    "                        # polygon: class id, x1, y1, x2, y2, x3, y3, x4, y4 (unnormalized)\n",
    "                        try:\n",
    "                            label = [catid_to_idx[object0['category_id']],\n",
    "                                     *np.array(multipoint.minimum_rotated_rectangle.exterior.coords[:-1]).ravel().tolist()]\n",
    "                            label, label_pixel = normalize_anchors(label, img['height'], img['width']) # normalize xyxyxyxy\n",
    "                            anno_txt.append(label)\n",
    "                            if plot: \n",
    "                                polygon_coords[-1].append(np.vstack((label_pixel[1:].reshape(-1, 2), label_pixel[1:3])))\n",
    "                                segment_coords[-1].append(segment)\n",
    "                        except Exception as e:\n",
    "                            print('Warning: Ignore label, ', e)\n",
    "                if plot: \n",
    "                    polygon_plot_image(img_name, polygon_coords, segment_coords)\n",
    "                    plot_now += 1\n",
    "                np.savetxt(str(anno_name), np.array(anno_txt), fmt=[\"%i\"]+[\"%.6f\"]*8)\n",
    "        with open(str(parent_path/(prefix+'.txt')), 'w+') as f:\n",
    "            for img_i, img_name in enumerate(img_txt):\n",
    "                if img_i == len(img_txt)-1: f.write(img_name)\n",
    "                else: f.write(img_name+'\\n')\n",
    "\n",
    "\n",
    "def normalize_anchors(label, img_h, img_w):\n",
    "    \"\"\"\n",
    "        polygon\n",
    "        FROM class id, x1, y1, x2, y2, x3, y3, x4, y4 (unnormalized)\n",
    "        TO class id (unchanged), x1, y1, x2, y2, x3, y3, x4, y4 (normalized to [0, 1])\n",
    "    \"\"\"\n",
    "    label = np.array(label)\n",
    "    label_pixel = np.copy(label)\n",
    "    label[1::2] = label[1::2]/img_w\n",
    "    label[2::2] = label[2::2]/img_h\n",
    "    # Common out the following lines to enable: polygon corners can be out of images\n",
    "    # label[1::2] = label[1::2].clip(0., img_w)/img_w\n",
    "    # label[2::2] = label[2::2].clip(0., img_h)/img_h\n",
    "    # label_pixel[1::2] = label_pixel[1::2].clip(0., img_w)\n",
    "    # label_pixel[2::2] = label_pixel[2::2].clip(0., img_h)\n",
    "    return label, label_pixel\n",
    "        \n",
    "def polygon_plot_image(img_name, polygon_coords, segment_coords=None):\n",
    "    img = mpimg.imread(img_name)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    if segment_coords is not None:\n",
    "        for seg_coo in segment_coords:\n",
    "            for segment in seg_coo:\n",
    "                plt.plot(*list(zip(*segment)))\n",
    "    for poly_coo in polygon_coords:\n",
    "        for polygon in poly_coo:\n",
    "            plt.plot(*list(zip(*polygon)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-7-2585ac2e63f4>\u001b[0m(42)\u001b[0;36mseg2poly\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     32 \u001b[0;31mdef seg2poly(dataset_path='',\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m             plot=True,):\n",
      "\u001b[0m\u001b[0;32m     34 \u001b[0;31m    \"\"\"\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m        \u001b[0mTransform\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpolygon\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m    \"\"\"\n",
      "\u001b[0m\u001b[0;32m     37 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m    \u001b[0;31m# Search for COCO json annotation files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m    \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# json files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     40 \u001b[0;31m    \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m    \u001b[0;32massert\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'dataset_path' should be a valid path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m    \u001b[0mf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'**'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m    \u001b[0;32massert\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error: no searched annotations files (.json) with {dataset_path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     45 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m    \u001b[0;31m# Iterate through each json_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mjson_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m        \u001b[0mcoco_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load coco data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m        \u001b[0mparent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0mimg_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# store searched image files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/2048_split\n",
      "/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/2048_split\n",
      "/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/2048_split\n",
      "/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/2048_split\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d039beb2ab01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseg2poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/2048_split'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2585ac2e63f4>\u001b[0m in \u001b[0;36mseg2poly\u001b[0;34m(dataset_path, plot)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'dataset_path' should be a valid path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'**'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error: no searched annotations files (.json) with {dataset_path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/images'"
     ]
    }
   ],
   "source": [
    "seg2poly('/mnt/Data/Project/ShipDetection/Data_Ship/Prepared/3m/2048_split', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a65e6cf362dcbd4c1e398f2eb7b8dbd88a924b26cc7f0f99c202a6a0f41e092a"
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch1_8': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
